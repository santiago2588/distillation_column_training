{
    "SE03": {
      "quiz_1": {
        "title": "Activation Functions Quiz",
        "question": "For a regression problem like predicting the robot arm joint angles, which activation function would be most appropriate for the output layer?",
        "options": [
          "Sigmoid - to constrain all outputs between 0 and 1",
          "ReLU - to ensure no negative values in the output",
          "Linear (no activation) - to allow any numeric output value",
          "Softmax - to convert outputs into probability distributions",
          "Tanh - to constrain all outputs between -1 and 1"
        ],
        "correct": 2,
        "explanation": "For regression problems, where we need to predict continuous values like joint angles, the linear activation (or no activation) is most appropriate for the output layer. This is because:\n\n1. Regression outputs need the full range of possible values, not constrained to intervals like [0,1] (sigmoid) or [-1,1] (tanh).\n2. Joint angles can be positive or negative, so ReLU (which outputs only positive values) would be too restrictive.\n3. Softmax is for multi-class classification problems, not regression.\n\nHidden layers in the network can and should use non-linear activations like ReLU, Tanh, or Leaky ReLU to capture complex relationships, but the output layer in regression tasks typically uses linear activation to predict any real-valued output."
      },
      "quiz_2": {
        "title": "Neural Network Architecture Quiz",
        "question": "When designing a neural network for the robot arm inverse kinematics problem, which combination of activation functions would likely work best?",
        "options": [
          "Hidden layers: Sigmoid, Output layer: Sigmoid",
          "Hidden layers: ReLU, Output layer: Linear",
          "Hidden layers: Linear, Output layer: ReLU",
          "Hidden layers: Softmax, Output layer: Tanh",
          "Hidden layers: Tanh, Output layer: Softmax"
        ],
        "correct": 1,
        "explanation": "The best combination for this regression task is:\n\nHidden layers: ReLU - ReLU (Rectified Linear Unit) works well in hidden layers because:\n1. It helps mitigate the vanishing gradient problem that can occur with sigmoid/tanh\n2. It introduces non-linearity needed to learn complex patterns\n3. It's computationally efficient\n4. It's widely used in modern neural networks with proven success\n\nOutput layer: Linear - Linear activation is appropriate for the output layer in regression tasks because:\n1. It allows the model to predict any numerical value within the range of joint angles\n2. Joint angles can be positive or negative values\n3. The model needs to predict exact values, not probabilities or classifications\n\nSigmoid and tanh would constrain outputs inappropriately, while softmax is designed for multi-class classification problems."
      },
      "quiz_3": {
        "title": "Neural Network Width Quiz",
        "question": "For the robot arm inverse kinematics regression problem, which statement about network width (number of neurons in hidden layers) is most accurate?",
        "options": [
          "A single neuron in each hidden layer is sufficient since this is a simple regression task",
          "The number of neurons should exactly match the number of input features (6)",
          "The hidden layers should have more neurons than inputs to capture complex spatial relationships",
          "The hidden layers should have fewer neurons than the output to prevent overfitting",
          "The width of the network doesn't impact performance, only the depth matters"
        ],
        "correct": 2,
        "explanation": "The hidden layers should have more neurons than inputs to capture complex spatial relationships in the robot arm kinematics problem. This is because:\n\n1. Inverse kinematics involves complex non-linear relationships between end-effector positions and joint angles\n2. The mapping from 6D input space (position and orientation) to 5D output space (joint angles) requires learning complex mathematical transformations\n3. With too few neurons, the network would suffer from high bias (underfitting)\n4. The number of neurons doesn't need to match the input or output dimensions exactly\n5. While having too many neurons can lead to overfitting, techniques like regularization and dropout can help mitigate this\n\nA common practice is to start with more neurons than inputs and gradually adjust based on validation performance."
      },
      "quiz_4": {
        "title": "Network Depth for Inverse Kinematics",
        "question": "When designing a neural network for robot arm inverse kinematics, which statement about network depth (number of hidden layers) is most accurate?",
        "options": [
          "A single hidden layer is always sufficient for any regression problem",
          "Deeper networks are always better than shallow ones for inverse kinematics",
          "Multiple hidden layers help capture hierarchical relationships in joint movements",
          "Deeper networks train faster than shallow networks",
          "The number of hidden layers should match the number of robot joints"
        ],
        "correct": 2,
        "explanation": "Multiple hidden layers help capture hierarchical relationships in joint movements. This is because:\n\n1. Inverse kinematics involves complex geometric and spatial transformations that benefit from hierarchical representations\n2. The first few layers might learn basic spatial features while deeper layers combine these into more complex movement patterns\n3. With a single hidden layer, the model might struggle to approximate the complex non-linear relationship between end-effector positions and joint configurations\n4. However, very deep networks may be prone to training difficulties like vanishing gradients\n5. The optimal depth depends on the complexity of the specific robot's kinematic chain, not just the number of joints\n\nDepth should be chosen based on the complexity of the task and validated empirically, not based on arbitrary rules."
      },
      "quiz_5": {
        "title": "Regularization Techniques for Kinematics Models",
        "question": "Which regularization technique would be most effective for improving generalization in a neural network for robot arm inverse kinematics?",
        "options": [
          "Using only linear layers without any non-linear activations",
          "Implementing dropout between layers during training",
          "Limiting training to exactly 10 epochs",
          "Using MSE loss instead of MAE loss",
          "Removing all hidden layers to simplify the model"
        ],
        "correct": 1,
        "explanation": "Implementing dropout between layers during training is most effective because:\n\n1. Dropout randomly deactivates a percentage of neurons during each training step, which prevents neurons from co-adapting too much\n2. This forces the network to learn redundant representations of the kinematic relationships\n3. The model becomes more robust to variations in input positions and orientations\n4. Dropout acts like an ensemble of different network architectures, improving generalization\n5. It helps prevent overfitting when the network needs to be expressive (with many parameters) to capture the complex mapping\n\nFor robot arm inverse kinematics, good generalization is critical since the robot needs to perform reliably across the entire workspace, even in positions not exactly represented in the training data."
      }
    },
    "SE04": {
        "quiz_1": {
            "title": "CNN Components Quiz",
            "question": "Which of the following statements about convolutional layers is TRUE?",
            "options": [
                "Convolutional layers always reduce the spatial dimensions of the input",
                "Each filter in a convolutional layer must have the same number of channels as the input",
                "The number of parameters in a convolutional layer depends on the size of the input image",
                "A 3x3 filter with stride 1 can only be applied to images with dimensions that are multiples of 3"
            ],
            "correct": 1,
            "explanation": "Each filter in a convolutional layer must have the same number of channels as the input. This is because the filter is applied across the entire depth of the input volume. The number of parameters in a convolutional layer depends on the filter size and number of filters, not on the input image size. Whether convolutional layers reduce dimensions depends on padding and stride settings. And filters can be applied to images of any dimension; they don't need to be multiples of the filter size."
        },
        "quiz_2": {
            "title": "CNN Architecture Quiz",
            "question": "Which CNN architecture introduced 'residual connections' to solve the vanishing gradient problem in very deep networks?",
            "options": [
                "AlexNet",
                "VGG-16",
                "ResNet",
                "Inception (GoogLeNet)"
            ],
            "correct": 2,
            "explanation": "ResNet (Residual Network) introduced residual connections (or skip connections) that allow gradients to flow directly through the network, helping to train very deep networks without suffering from vanishing gradients. This innovation enabled the successful training of networks with over 100 layers. AlexNet was the first CNN to win ImageNet, VGG-16 used small filters throughout the network, and Inception used modules with multiple filter sizes in parallel."
        },
        "quiz_3": {
            "title": "CNN Training Best Practices",
            "question": "When training a CNN from scratch with limited data, which approach is MOST likely to improve generalization?",
            "options": [
                "Using a very large model with many parameters",
                "Applying aggressive data augmentation techniques",
                "Removing all regularization methods like dropout",
                "Training with a very high learning rate"
            ],
            "correct": 1,
            "explanation": "With limited data, applying data augmentation techniques (like random crops, flips, rotations, etc.) is one of the most effective ways to improve model generalization. This artificially expands your dataset by creating variations of the existing images. Using a very large model with many parameters would likely lead to overfitting on a small dataset. Removing regularization would also lead to overfitting, and using a very high learning rate would likely cause training instability."
        },
        "quiz_4": {
            "title": "Pooling Layers in CNNs",
            "question": "What is the primary purpose of pooling layers in CNNs?",
            "options": [
                "To introduce non-linearity into the network",
                "To extract features from the input image",
                "To reduce spatial dimensions and provide translation invariance",
                "To normalize pixel values across all feature maps"
            ],
            "correct": 2,
            "explanation": "The primary purpose of pooling layers is to reduce the spatial dimensions of the feature maps and provide some degree of translation invariance. By downsampling the feature maps, pooling helps make the network robust to small variations in the position of features. Non-linearity is introduced by activation functions (like ReLU), feature extraction is done by convolutional layers, and normalization is typically handled by batch normalization layers."
        },
        "quiz_5": {
            "title": "Kernel Size Selection",
            "question": "When should you consider using larger kernel sizes (e.g., 5×5 or 7×7) instead of 3×3 kernels in a CNN?",
            "options": [
                "When you need to reduce computation time",
                "When processing high-resolution images and need to capture larger spatial patterns",
                "When your dataset has very few samples",
                "When training on a CPU instead of a GPU"
            ],
            "correct": 1,
            "explanation": "Larger kernel sizes are beneficial when you need to capture larger spatial patterns in high-resolution images. They increase the receptive field size more rapidly, allowing the network to 'see' more of the input image at once. However, they also increase computational cost (not reduce it), require more training data (not less), and are more computationally intensive on any hardware platform including CPUs."
        }
    },
    "SE05": {
      "quiz_1": {
        "title": "Transfer Learning Applications Quiz",
        "question": "In which engineering scenario would transfer learning provide the MOST significant advantage?",
        "options": [
          "When you have a simple classification problem with millions of labeled examples",
          "When you need to detect defects in industrial parts but only have 200 labeled images",
          "When you're building a model to predict well-documented physical properties",
          "When computational efficiency isn't a concern and you have months to train a model from scratch"
        ],
        "correct": 1,
        "explanation": "Transfer learning provides a major advantage for engineering applications by allowing knowledge learned from one task to be applied to a different but related task. This approach significantly reduces the amount of training data required and accelerates the training process, which is particularly valuable in engineering domains where labeled datasets may be limited or expensive to obtain. Pre-trained models have already learned useful features from large datasets like ImageNet, allowing engineers to build effective models for specialized applications without starting from scratch."
      },
      "quiz_2": {
        "title": "Feature Extraction vs Fine-Tuning Quiz",
        "question": "For a materials science dataset with only 500 samples of microstructure images, which transfer learning approach would likely be most appropriate?",
        "options": [
          "Training from scratch (no transfer learning)",
          "Fine-tuning all layers of a pre-trained model",
          "Feature extraction with a frozen pre-trained backbone",
          "Using the pre-trained model without any modifications"
        ],
        "correct": 2,
        "explanation": "Feature extraction with a frozen pre-trained backbone is typically the best approach for small datasets like the materials science example with only 500 samples. When using feature extraction, the pre-trained network layers are kept frozen (weights aren't updated), and only the new classification layers are trained. This approach prevents overfitting on the small dataset while still leveraging the powerful feature representations learned from a large dataset. Training from scratch would likely lead to overfitting, while full fine-tuning might also overfit with so little data. Using the pre-trained model without modifications wouldn't adapt to the specific materials science task at all."
      },
      "quiz_3": {
        "title": "Pre-trained Model Selection Quiz",
        "question": "What is the MOST important factor to consider when selecting a pre-trained model for a transfer learning project?",
        "options": [
          "The original training dataset's similarity to your target application",
          "How recently the model was published",
          "The model's size and inference speed",
          "The popularity of the model in research papers"
        ],
        "correct": 0,
        "explanation": "The similarity between the original training data and your target application is the most important factor when selecting a pre-trained model for transfer learning. Models trained on data similar to your target domain will have learned more relevant features, making transfer learning more effective. While inference speed, model size, and recency are all relevant considerations, the domain similarity directly impacts how well the learned features will transfer to your engineering task. For example, a model pre-trained on natural images may transfer well to structural crack detection, but less well to specialized domains like hyperspectral satellite imagery or medical scans."
      },
      "quiz_4": {
        "title": "Transfer Learning Techniques Quiz",
        "question": "When progressively unfreezing layers during fine-tuning, what is the correct order to follow?",
        "options": [
          "Unfreeze earlier layers first, then gradually unfreeze later layers",
          "Unfreeze all layers at once with the same learning rate",
          "Unfreeze later layers first, then gradually unfreeze earlier layers",
          "Keep all layers frozen and only train the classifier head"
        ],
        "correct": 2,
        "explanation": "The correct order for progressive unfreezing is to unfreeze later (deeper) layers first, then gradually unfreeze earlier layers. This approach is based on the understanding that early convolutional layers in a CNN capture generic features (like edges and textures) that transfer well across domains, while later layers capture more task-specific features. By unfreezing and fine-tuning the later layers first, we adapt the model to the new task while preserving the useful generic features. This strategy helps prevent catastrophic forgetting and typically leads to better performance compared to unfreezing all layers simultaneously or in the reverse order."
      },
      "quiz_5": {
        "title": "Learning Rate Selection Quiz",
        "question": "When fine-tuning a pre-trained model, what learning rate strategy is typically most effective?",
        "options": [
          "Use the same high learning rate for all layers to quickly adapt the model",
          "Use a much smaller learning rate than for training from scratch",
          "Use a higher learning rate for early layers than for later layers",
          "Learning rates don't matter much in transfer learning"
        ],
        "correct": 1,
        "explanation": "When fine-tuning a pre-trained model, using a much smaller learning rate than you would for training from scratch is typically most effective. This is because pre-trained weights already encode valuable information, and large learning rates could destroy this information through drastic updates. A common practice is to use a learning rate that is 10-100 times smaller than what you would use for training from scratch. This allows the model to make small adjustments to adapt to the new task while preserving the useful features learned during pre-training. Additionally, some practitioners use even smaller learning rates for earlier layers (which contain more general features) and slightly larger ones for later layers (which need more adaptation to the specific task)."
      }
    }
}